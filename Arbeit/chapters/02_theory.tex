% ============================================================================
% KAPITEL 2: THEORETICAL FOUNDATIONS
% Fokussierter Theorieteil für Masterarbeit (20-25 Seiten)
% ============================================================================

\chapter{Theoretical Foundations}
\label{ch:theory}

This chapter establishes the theoretical foundations necessary to understand the central challenge of this thesis: predicting multi-crystal sedimentation flows using machine learning. We first review the physics of crystal settling in Stokes flow regimes (\S\ref{sec:crystal_physics}), then examine how UNet architectures enable efficient flow field prediction (\S\ref{sec:unet}), discuss physics-informed training approaches (\S\ref{sec:pinns}), and finally analyze the fundamental generalization challenge when scaling from single to multiple crystals (\S\ref{sec:generalization}).

% ============================================================================
\section{Physics of Crystal Sedimentation}
\label{sec:crystal_physics}
% ============================================================================

\subsection{Stokes Flow Fundamentals}

Crystal sedimentation in magmatic systems occurs at very low Reynolds numbers ($\text{Re} \ll 1$), placing the flow firmly in the Stokes regime where inertial effects are negligible compared to viscous forces. The governing equations reduce from the full Navier-Stokes equations to the Stokes equations:

\begin{align}
\mathbf{0} &= -\nabla p + \mu \nabla^2 \mathbf{u} \label{eq:stokes_momentum}\\
\nabla \cdot \mathbf{u} &= 0 \label{eq:stokes_continuity}
\end{align}

where $p$ is pressure, $\mu$ is dynamic viscosity, and $\mathbf{u} = (u, v)$ is the velocity field. The Reynolds number is defined as:

\begin{equation}
\text{Re} = \frac{\rho_f v_p D}{\mu}
\end{equation}

where $\rho_f$ is fluid density, $v_p$ is particle velocity, and $D$ is particle diameter. Magmatic systems typically exhibit $\text{Re} \sim 10^{-4}$ to $10^{-2}$ \cite{martin1988crystal,martin1989fluid}.

\textbf{Key physical parameters} for olivine crystals in basaltic magma:
\begin{itemize}
    \item Crystal density contrast: $\Delta\rho \sim 600$ kg/m$^3$
    \item Magma viscosity: $\mu \sim 10^1$ to $10^3$ Pa·s
    \item Crystal diameter: $D \sim 0.1$ to 10 mm
    \item Settling velocities: $v_s \sim 10^{-8}$ to $10^{-4}$ m/s
\end{itemize}

For an isolated spherical crystal, Stokes' law provides the terminal settling velocity:

\begin{equation}
v_s = \frac{1}{18} \cdot \frac{(\rho_p - \rho_f)}{\mu} \cdot g \cdot D^2
\label{eq:stokes_law}
\end{equation}

This velocity results from the balance of three forces: gravitational ($F_g = \frac{4}{3}\pi R^3 \rho_p g$), buoyancy ($F_b = \frac{4}{3}\pi R^3 \rho_f g$), and drag ($F_d = 6\pi\mu R v$).

\subsection{Multi-Particle Interactions and Collective Dynamics}

When multiple crystals are present, the flow field becomes significantly more complex due to hydrodynamic interactions. These interactions occur through two primary mechanisms:

\textbf{Far-field interactions} are described by the Oseen tensor, with velocity disturbances decaying as $1/r$ from each particle. While individually weak, these long-range effects become significant when many particles are present.

\textbf{Near-field lubrication forces} dominate when particles approach closely ($\delta \ll R$, where $\delta$ is the gap size). These forces scale approximately as $F \sim \mu V R/\delta$ and prevent direct particle contact while dramatically altering local flow patterns.

A particularly important collective phenomenon is the \textbf{drafting-kissing-tumbling (DKT) sequence} observed when two particles settle vertically \cite{fortes1987nonlinear}:

\begin{enumerate}
    \item \textbf{Drafting}: The trailing particle accelerates in the wake of the leading particle due to reduced drag
    \item \textbf{Kissing}: Particles approach closely, entering the lubrication regime
    \item \textbf{Tumbling}: Symmetry breaks, particles separate laterally and can exchange roles
\end{enumerate}

This DKT cycle serves as a critical validation benchmark for multi-particle simulations and demonstrates emergent behavior not predictable from single-particle physics alone.

\textbf{Hindered settling} occurs at higher particle concentrations, where the mean settling velocity decreases according to the Richardson-Zaki correlation \cite{happel1983low}:

\begin{equation}
\frac{v}{v_0} = (1-\phi)^n
\label{eq:richardson_zaki}
\end{equation}

where $\phi$ is the solid volume fraction and $n \approx 5$ to 6 for Stokes flow. At dilute concentrations ($\phi < 3\%$), Batchelor's theory gives:

\begin{equation}
\frac{v}{v_0} = 1 - 6.55\phi
\end{equation}

\subsection{Scaling from Single to Multiple Crystals}

The transition from single-crystal to multi-crystal systems introduces several critical challenges:

\textbf{Interaction complexity scaling}: While a single crystal has zero interactions with other crystals, $N$ crystals have $N(N-1)/2$ pairwise interactions. For 15 crystals, this represents 105 potential pairwise interactions, along with higher-order multi-body effects.

\textbf{Wake interference}: Individual crystal wakes can overlap and interact in complex ways, creating collective flow patterns not present in single-crystal cases. At sufficient particle concentrations (Galileo number $\text{Ga} \geq 178$), clustering occurs with columnar structures forming and velocity enhancements up to 12\% \cite{uhlmann2014clustering}.

\textbf{Configuration sensitivity}: The spatial arrangement of crystals critically affects the dynamics. Vertical alignment promotes DKT behavior, while side-by-side arrangements lead to mutual repulsion. This sensitivity makes the configuration space extremely high-dimensional and difficult to sample comprehensively.

\textbf{Critical insight for this thesis}: Understanding flows around 1 to 15 crystals requires capturing not just individual particle physics, but also the compositional rules by which these individual effects combine into collective behavior.

% ============================================================================
\section{UNet Architecture for Flow Field Prediction}
\label{sec:unet}
% ============================================================================

\subsection{From CNNs to UNet: Motivation for Physics}

Convolutional Neural Networks (CNNs) have emerged as powerful tools for predicting spatial fields in fluid dynamics due to several key properties:

\begin{itemize}
    \item \textbf{Translation invariance}: Physics laws are spatially homogeneous, matching CNN's shift-invariant kernels
    \item \textbf{Local receptive fields}: Capture local spatial correlations in flow fields
    \item \textbf{Parameter efficiency}: $10^2$ to $10^3\times$ fewer parameters than fully-connected networks for equivalent capacity
    \item \textbf{Hierarchical features}: Multi-scale representations naturally emerge through pooling layers
\end{itemize}

However, standard CNNs with only encoder structures struggle with pixel-accurate spatial predictions required for flow fields. This limitation motivated the development of encoder-decoder architectures, culminating in the UNet.

\subsection{UNet Architecture and Skip Connections}

The UNet architecture, introduced by Ronneberger et al. \cite{ronneberger2015unet}, combines a contracting encoder path with an expanding decoder path, connected by skip connections at each resolution level (Figure~\ref{fig:unet_architecture}).

\begin{figure}[htbp]
    \centering
    % Replace with your actual figure
    \includegraphics[width=0.85\textwidth]{figures/unet_architecture.pdf}
    \caption{UNet architecture showing encoder (left), bottleneck (bottom), decoder (right), and skip connections (horizontal arrows). The encoder progressively reduces spatial resolution while increasing feature depth. Skip connections preserve fine-grained spatial information essential for accurate flow field reconstruction.}
    \label{fig:unet_architecture}
\end{figure}

\textbf{Encoder path} consists of repeated blocks of:
\begin{itemize}
    \item Two $3 \times 3$ convolutions with ReLU activation
    \item $2 \times 2$ max pooling for downsampling
    \item Feature channels double at each level: $64 \rightarrow 128 \rightarrow 256 \rightarrow 512$
\end{itemize}

The encoder progressively reduces spatial dimensions while increasing feature depth, building increasingly abstract representations that capture global context.

\textbf{Decoder path} mirrors the encoder with:
\begin{itemize}
    \item $2 \times 2$ upsampling (transposed convolutions)
    \item Concatenation with corresponding encoder features via skip connections
    \item Two $3 \times 3$ convolutions with ReLU
\end{itemize}

The \textbf{bottleneck} at the coarsest resolution (typically $1024$ channels) represents the most compressed representation, capturing the global structure of the flow field.

\textbf{Skip connections} are the critical innovation that distinguishes UNet from simple encoder-decoder architectures. By concatenating encoder features directly to decoder features at each resolution level, skip connections:

\begin{enumerate}
    \item \textbf{Preserve spatial information}: Fine-grained details lost during downsampling are recovered
    \item \textbf{Combine multi-scale features}: High-level semantic information from the decoder merges with low-level spatial details from the encoder
    \item \textbf{Enable gradient flow}: Direct paths facilitate backpropagation through very deep networks
    \item \textbf{Maintain sharp boundaries}: Essential for capturing no-slip conditions at crystal surfaces and sharp velocity gradients
\end{enumerate}

\subsection{Why UNet Excels for Fluid Dynamics}

UNet's architecture aligns exceptionally well with the requirements of fluid dynamics prediction:

\textbf{Multi-scale phenomena}: Fluid flows naturally exhibit hierarchical structure---from large-scale convection patterns to small-scale boundary layers and wakes. UNet's pyramid structure with feature maps at multiple resolutions directly captures this hierarchy.

\textbf{Boundary preservation}: The no-slip boundary condition at crystal surfaces creates sharp velocity gradients that must be accurately represented. Skip connections preserve these fine spatial details that would otherwise be lost in the encoder bottleneck.

\textbf{Computational efficiency}: Despite operating on high-resolution grids ($256 \times 256$ or $512 \times 512$), UNet achieves fast inference ($<1$s) through its bottleneck compression, making real-time or interactive applications feasible.

\textbf{Proven track record}: UNet has demonstrated exceptional performance for physics simulations:
\begin{itemize}
    \item Ribeiro et al. \cite{ribeiro2019unet_flow} achieved accurate laminar flow predictions around arbitrary geometries
    \item Thuerey et al. \cite{thuerey2020deep} obtained 40--80$\times$ speedup over traditional CFD with comparable accuracy for airfoil flows
    \item Hou et al. \cite{hou2022unet_lstm} demonstrated order-of-magnitude improvements in MSE/MAE with $10^6\times$ speedup for submarine hydrodynamics time-series
\end{itemize}

\subsection{UNet Variants for Physics Applications}

Several UNet variants have been developed to further improve performance for physics simulations:

\textbf{Gated Residual UNet} (Rana et al. \cite{rana2024scalable_cnn}): Incorporates gated residual units with ELU activations, achieving $\sim$18\% improvement over standard UNet for fluid dynamics. This architecture was specifically designed for flow prediction and serves as the basis for the scalable domain decomposition approach.

\textbf{Attention UNet}: Adds attention gates in skip connections to selectively emphasize relevant spatial regions. Particularly effective for focusing on shock fronts, boundary layers, and regions with sharp gradients, with $\sim$12\% improvement demonstrated.

\textbf{UNet-LSTM Hybrid}: Combines UNet spatial feature extraction with LSTM temporal sequence modeling (Hou et al. \cite{hou2022unet_lstm}). Essential when predicting time-dependent flows or crystal trajectories over multiple timesteps.

For this thesis, the gated residual UNet architecture provides the best balance of performance and computational efficiency for steady or quasi-steady flow prediction around multiple crystals.

\subsection{Domain Decomposition for Scalability}

A critical recent development highly relevant to this thesis is Rana et al.'s \cite{rana2024scalable_cnn} demonstration that UNets trained on single-obstacle flows can scale to arbitrary domain sizes through domain decomposition.

\textbf{Key insight}: Train a local model on small subdomains containing 1--3 obstacles, then apply it repeatedly across larger domains with many obstacles. This approach directly addresses the central challenge of this thesis: generalizing from training on few crystals to predicting flows around many crystals.

\textbf{Two-stage approach}:
\begin{enumerate}
    \item \textbf{Local model}: Predict flow fields on overlapping subdomains ($128 \times 128$ pixels)
    \item \textbf{Continuous model}: A second shallow network smooths boundary discontinuities between subdomain predictions, reducing MSE by 18\%
\end{enumerate}

\textbf{LOVE metric} (Local Orientational Vector Field Entropy) determines optimal subdomain size by computing Shannon entropy of velocity orientations:
\begin{equation}
H = -\sum_i p_i \log(p_i)
\end{equation}
The subdomain size is chosen where $H$ plateaus, indicating the decorrelation length scale beyond which the flow returns to an unperturbed state.

\textbf{Scalability results}: Rana et al. demonstrated successful scaling up to $1280 \times 640$ pixels (10$\times$ training domain size) with linear error propagation: $\text{MSE}(n) = n \cdot \Delta\varepsilon$. Critically, this was achieved using Reynolds numbers 2--350, spanning from Stokes to moderate-Reynolds regimes.

\textbf{Relevance for crystal sedimentation}: This work provides direct precedent that single-object training can successfully generalize to multiple objects through spatial decomposition---exactly the scaling challenge addressed in this thesis.

% ============================================================================
\section{Physics-Informed Neural Networks}
\label{sec:pinns}
% ============================================================================

\subsection{Motivation and Framework}

While purely data-driven neural networks can achieve impressive accuracy, they face fundamental limitations:
\begin{itemize}
    \item Require large training datasets (thousands to millions of samples)
    \item Poor extrapolation beyond training distribution
    \item No guarantee of physical consistency (conservation laws, boundary conditions)
    \item Can produce physically implausible predictions
\end{itemize}

Physics-Informed Neural Networks (PINNs) address these limitations by incorporating governing equations directly into the training process \cite{raissi2019physics}. Rather than learning purely from input-output pairs, the network is constrained to respect known physical laws.

\subsection{Loss Function Structure}

The total loss function combines data-driven and physics-based terms:

\begin{equation}
\mathcal{L}_{\text{total}} = \mathcal{L}_{\text{data}} + \lambda_{\text{PDE}} \cdot \mathcal{L}_{\text{PDE}} + \lambda_{\text{BC}} \cdot \mathcal{L}_{\text{BC}} + \lambda_{\text{IC}} \cdot \mathcal{L}_{\text{IC}}
\label{eq:pinn_loss}
\end{equation}

\textbf{Data loss} ($\mathcal{L}_{\text{data}}$): Standard supervised loss on labeled training data
\begin{equation}
\mathcal{L}_{\text{data}} = \frac{1}{N_{\text{data}}} \sum_{i=1}^{N_{\text{data}}} \|\mathbf{u}_{\text{pred}}(\mathbf{x}_i) - \mathbf{u}_{\text{true}}(\mathbf{x}_i)\|^2
\end{equation}

\textbf{PDE loss} ($\mathcal{L}_{\text{PDE}}$): Residuals of governing equations evaluated at collocation points
\begin{equation}
\mathcal{L}_{\text{PDE}} = \frac{1}{N_{\text{col}}} \sum_{i=1}^{N_{\text{col}}} \|\mathcal{F}[\mathbf{u}_{\text{pred}}](\mathbf{x}_i)\|^2
\end{equation}
where $\mathcal{F}$ represents the PDE operator. For steady Stokes flow:
\begin{equation}
\mathcal{F}[\mathbf{u}] = \nabla p - \mu \nabla^2 \mathbf{u}
\end{equation}

\textbf{Boundary condition loss} ($\mathcal{L}_{\text{BC}}$): Enforces no-slip at crystal surfaces and prescribed conditions at domain boundaries
\begin{equation}
\mathcal{L}_{\text{BC}} = \frac{1}{N_{\text{BC}}} \sum_{i=1}^{N_{\text{BC}}} \|\mathbf{u}_{\text{pred}}(\mathbf{x}_i^{\text{boundary}}) - \mathbf{u}_{\text{prescribed}}\|^2
\end{equation}

\textbf{Initial condition loss} ($\mathcal{L}_{\text{IC}}$): For time-dependent problems, matches initial state

The weights $\lambda_{\text{PDE}}, \lambda_{\text{BC}}, \lambda_{\text{IC}}$ balance the different loss components. Choosing these weights is non-trivial, as discussed below.

\subsection{Advantages for Fluid Dynamics}

\textbf{Data efficiency}: Physics constraints act as regularization, dramatically reducing required training data. Studies show 100--1000 samples can suffice with physics-informed training versus 10,000+ for purely data-driven approaches \cite{raissi2019physics}.

\textbf{Physical consistency}: The network is explicitly trained to satisfy conservation laws:
\begin{itemize}
    \item Mass conservation (continuity): $\nabla \cdot \mathbf{u} = 0$
    \item Momentum conservation (Stokes equations)
    \item Boundary conditions (no-slip at solid surfaces)
\end{itemize}

\textbf{Improved extrapolation}: By learning the underlying physics rather than just data patterns, physics-informed networks generalize better to unseen configurations. This is critical for the 1$\rightarrow$15 crystal scaling challenge.

\textbf{Inverse problems}: PINNs enable parameter estimation (e.g., viscosity, boundary conditions) from sparse measurements---not directly relevant to this thesis but demonstrates the framework's flexibility.

\subsection{Implementation for Stokes Flow}

For this thesis focusing on Stokes flow around crystals, the physics-informed loss components are:

\textbf{Continuity loss} (divergence-free constraint):
\begin{equation}
\mathcal{L}_{\text{div}} = \frac{1}{N_{\text{col}}} \sum_{i=1}^{N_{\text{col}}} \left(\frac{\partial u}{\partial x} + \frac{\partial v}{\partial z}\right)^2
\label{eq:continuity_loss}
\end{equation}

\textbf{Momentum loss}:
\begin{equation}
\mathcal{L}_{\text{mom}} = \frac{1}{N_{\text{col}}} \sum_{i=1}^{N_{\text{col}}} \left\|\nabla p - \mu \nabla^2 \mathbf{u}\right\|^2
\end{equation}

\textbf{No-slip boundary condition}:
\begin{equation}
\mathcal{L}_{\text{BC}} = \frac{1}{N_{\text{BC}}} \sum_{i=1}^{N_{\text{BC}}} \|\mathbf{u}(\mathbf{x}_i^{\text{crystal}})\|^2
\end{equation}

Computing these losses requires automatic differentiation to evaluate spatial derivatives $\partial u/\partial x$, $\nabla^2 u$, etc. Modern deep learning frameworks (PyTorch, TensorFlow) provide efficient automatic differentiation, though this adds computational cost during training.

\subsection{Challenges and Solutions}

\textbf{Gradient imbalance problem}: Different loss terms can have vastly different magnitudes ($\mathcal{L}_{\text{data}} \sim 10^{-2}$, $\mathcal{L}_{\text{PDE}} \sim 10^4$), causing training instability.

\textbf{Solutions}:
\begin{itemize}
    \item \textbf{Adaptive weighting}: Update $\lambda$ values during training to balance gradient magnitudes. Self-adaptive loss balancing (lbPINNs) uses probabilistic models to automatically adjust weights
    \item \textbf{Neural Tangent Kernel (NTK) approach}: Weight terms based on their gradient magnitudes to ensure balanced learning
    \item \textbf{Curriculum learning}: Start with high data loss weight, gradually increase physics loss weight (see Table~\ref{tab:lambda_schedule} in Chapter~\ref{ch:methodology})
\end{itemize}

\textbf{Collocation point placement}: The choice of where to evaluate PDE residuals affects learning. Adaptive sampling strategies add points in regions with high residuals.

\textbf{Computational cost}: Automatic differentiation and PDE residual computation add overhead. Training time increases by 2--5$\times$ compared to purely data-driven approaches, but inference remains fast.

\textbf{For Stokes flow} (this thesis): The low Reynolds number regime actually makes physics-informed training easier than for turbulent flows, as the equations are linear and lack stiffness issues that plague high-Re simulations.

\subsection{Alternative: Physics-Based Loss Without Full PINNs}

A lighter-weight alternative to full PINNs is to include physics-based loss terms without computing full PDE residuals \cite{thuerey2020deep}:

\begin{equation}
\mathcal{L}_{\text{physics}} = \alpha_{\text{div}} \mathcal{L}_{\text{div}} + \alpha_{\text{grad}} \mathcal{L}_{\text{grad}}
\end{equation}

where $\mathcal{L}_{\text{grad}}$ penalizes discontinuities in velocity gradients (reduces spurious oscillations). This approach:
\begin{itemize}
    \item Requires fewer derivative computations
    \item Trains faster than full PINNs
    \item Still enforces key physical constraints (continuity)
    \item Achieved 40--80$\times$ speedup in Deep Fluids framework \cite{thuerey2020deep}
\end{itemize}

This "soft physics-informed" approach represents a practical middle ground between purely data-driven and full PINN training, and is the approach adopted in this thesis (see Chapter~\ref{ch:methodology}).

% ============================================================================
\section{The Generalization Challenge}
\label{sec:generalization}
% ============================================================================

\subsection{Interpolation vs. Extrapolation in Neural Networks}

A fundamental distinction in machine learning is between interpolation and extrapolation:

\textbf{Interpolation}: Predictions within the convex hull of the training data distribution. Neural networks excel at interpolation, achieving high accuracy when test examples are similar to training examples.

\textbf{Extrapolation}: Predictions outside the training data distribution. Neural networks notoriously struggle with extrapolation, often producing unreliable or physically implausible results.

The asymptotic behavior of neural networks outside the training regime is determined primarily by the activation functions (ReLU, sigmoid, tanh), not by the training data. This makes extrapolation fundamentally unreliable without additional constraints.

\textbf{Critical for this thesis}: Training on 1--5 crystals and testing on 6--15 crystals is \textbf{extrapolation in configuration space}, not interpolation. The network must learn compositional rules to combine single-crystal physics into multi-crystal scenarios, rather than memorizing specific configurations.

\subsection{Configuration Space Scaling}

The challenge becomes clear when examining the configuration space dimensionality:

\textbf{Single crystal}: 3 degrees of freedom (DOF)---position $(x, y)$ and orientation $\theta$. The network learns flow patterns for one obstacle in various positions.

\textbf{15 crystals}: 45 DOF---15 positions plus 15 orientations. The configuration space is 15-dimensional, and cannot be densely sampled.

\textbf{Interaction complexity}: The number of pairwise interactions scales quadratically:
\begin{itemize}
    \item 1 crystal: 0 interactions
    \item 2 crystals: 1 interaction (DKT)
    \item 5 crystals: 10 interactions
    \item 15 crystals: 105 interactions
\end{itemize}

The network must learn to predict 105 simultaneous interactions after being trained on far fewer. This requires \textbf{compositional generalization}---the ability to combine learned primitives in novel ways.

\subsection{Compositional Generalization}

\textbf{Definition}: The ability to understand and produce novel combinations from known components in systematic, algebraic ways \cite{lake2023systematic}.

\textbf{The challenge for neural networks}:
\begin{itemize}
    \item Standard architectures lack explicit mechanisms for systematic composition
    \item Tend to memorize patterns rather than learning underlying rules
    \item Struggle with combinatorial generalization to unseen configurations
\end{itemize}

\textbf{Successful approach} (Lake \& Baroni \cite{lake2023systematic}): Training on diverse compositional tasks enables systematic generalization even with standard architectures (transformers). Key insight: \textit{diversity of training tasks matters more than architecture}.

\textbf{Implication for this thesis}: Training on diverse crystal configurations (1, 2, 3, 5 crystals in various arrangements) may enable compositional generalization to 15 crystals, even without explicit compositional mechanisms in the architecture.

\subsection{Transfer Learning and Curriculum Learning}

Two complementary strategies address the generalization challenge:

\textbf{Transfer learning}: Pre-train on abundant simple data, fine-tune on scarce complex data \cite{pellegrin2022transfer}:
\begin{enumerate}
    \item Train UNet on 10,000 single-crystal configurations
    \item Fine-tune on 5,000 two-crystal configurations (freeze early layers)
    \item Fine-tune on 2,000 three- to five-crystal configurations (selective unfreezing)
    \item Fine-tune on 500 ten- to fifteen-crystal configurations (all layers trainable)
\end{enumerate}

\textbf{Layer-wise adaptation}:
\begin{itemize}
    \item \textbf{Early layers}: Low-level features (edges, boundaries)---transfer well across crystal counts
    \item \textbf{Middle layers}: Mid-level features (individual wakes)---partially transferable
    \item \textbf{Late layers}: High-level features (multi-body interactions)---require retraining
\end{itemize}

Transfer learning has demonstrated 97.3\% performance improvement for physics-informed networks \cite{pellegrin2022transfer}, dramatically reducing data requirements for complex scenarios.

\textbf{Curriculum learning}: Progressive difficulty $1 \rightarrow 2 \rightarrow 3 \rightarrow 5 \rightarrow 10 \rightarrow 15$ crystals. Each stage reduces the extrapolation gap, transforming one large extrapolation step into multiple smaller ones.

\subsection{Domain Decomposition as Generalization Strategy}

Rana et al.'s \cite{rana2024scalable_cnn} domain decomposition approach (discussed in \S\ref{sec:unet}) provides another generalization mechanism:

Rather than learning ``15-crystal physics'' as a distinct concept, decompose the problem into ``local 1--3 crystal physics'' applied repeatedly. The network learns:
\begin{itemize}
    \item Flow patterns around individual crystals
    \item How these patterns decay with distance (via LOVE metric decorrelation length)
    \item How to stitch local predictions into a global field
\end{itemize}

This reframes generalization from ``predict novel N-body configurations'' to ``apply learned local physics at multiple locations''---a significantly easier problem.

\textbf{Key advantage}: Training data requirements scale with subdomain complexity, not full-domain complexity. A model trained on 1--3 crystals can handle 15+ crystals without retraining.

\subsection{Metrics for Evaluating Generalization}

To quantify generalization performance, we employ multiple metrics:

\textbf{Standard metrics}:
\begin{itemize}
    \item Mean Squared Error (MSE) and Mean Absolute Error (MAE) for velocity/pressure fields
    \item $R^2$ coefficient of determination
    \item Relative errors: $\|\mathbf{u}_{\text{pred}} - \mathbf{u}_{\text{true}}\| / \|\mathbf{u}_{\text{true}}\|$
\end{itemize}

\textbf{Physics-based metrics}:
\begin{itemize}
    \item Divergence error: $\|\nabla \cdot \mathbf{u}\|$ (should be $\approx 0$ everywhere)
    \item Boundary condition violations: $\|\mathbf{u}(\mathbf{x}_{\text{crystal}})\|$ at no-slip surfaces
    \item Momentum balance: Comparison of forces on crystals with hydrodynamic predictions
\end{itemize}

\textbf{Generalization-specific metrics}:
\begin{itemize}
    \item \textbf{Extrapolation error}: Performance on $N > N_{\text{train}}$ crystals versus $N \leq N_{\text{train}}$
    \item \textbf{Transfer learning gain}: Improvement from pre-training versus training from scratch
    \item \textbf{Systematic generalization tests}: Novel spatial arrangements (e.g., regular lattices if trained on random configurations)
\end{itemize}

\textbf{Benchmark validation}:
\begin{itemize}
    \item DKT sequence for 2 crystals (qualitative behavior check)
    \item Richardson-Zaki hindered settling for multiple crystals (quantitative scaling)
    \item Comparison with high-fidelity LBM-DEM simulations
\end{itemize}

\subsection{Theoretical Bounds and Expectations}

Based on the literature review, we can establish theoretical expectations for generalization performance:

\textbf{With purely data-driven training}: Poor extrapolation is expected. Networks trained only on 1--5 crystals will likely fail catastrophically on 15 crystals, producing physically implausible flow fields.

\textbf{With physics-informed constraints}: Moderate extrapolation improvement expected. Enforcing $\nabla \cdot \mathbf{u} = 0$ and boundary conditions should maintain physical plausibility, but accuracy may degrade.

\textbf{With transfer learning}: Significant improvement expected. The 97\% boost demonstrated by Pellegrin et al. \cite{pellegrin2022transfer} suggests this is the most promising single strategy.

\textbf{With domain decomposition}: Best extrapolation expected. Rana et al.'s \cite{rana2024scalable_cnn} results suggest this approach should enable reliable scaling from few to many crystals.

\textbf{With hybrid approach} (physics-informed + transfer + domain decomposition): This combination should provide the most robust generalization, and is the approach adopted in this thesis (see Chapter~\ref{ch:methodology}).

% ============================================================================
\section{Summary}
\label{sec:theory_summary}
% ============================================================================

This chapter established four theoretical pillars for the thesis:

\begin{enumerate}
    \item \textbf{Crystal sedimentation physics} (\S\ref{sec:crystal_physics}): Stokes flow regime ($\text{Re} \ll 1$) enables linear superposition, but N-body effects emerge from collective hydrodynamic coupling. Multi-particle interactions scale quadratically with crystal count, introducing complexity not present in single-crystal systems.

    \item \textbf{UNet architecture} (\S\ref{sec:unet}): The encoder-decoder structure with skip connections is ideally suited for flow field prediction, capturing multi-scale phenomena while preserving sharp spatial features. Rana et al.'s \cite{rana2024scalable_cnn} domain decomposition approach provides direct precedent for scaling from single to multiple obstacles.

    \item \textbf{Physics-informed training} (\S\ref{sec:pinns}): Incorporating physical constraints (continuity equation, boundary conditions) into the loss function improves data efficiency and extrapolation while ensuring physical plausibility. For Stokes flow, this is particularly tractable due to equation linearity.

    \item \textbf{Generalization challenge} (\S\ref{sec:generalization}): Scaling from 1--5 to 6--15 crystals is extrapolation in high-dimensional configuration space (3D$\rightarrow$45D), requiring compositional learning rather than interpolation. Transfer learning, curriculum learning, and domain decomposition provide complementary strategies to address this fundamental challenge.
\end{enumerate}

The methodology in Chapter~\ref{ch:methodology} combines these theoretical insights into a hybrid approach: physics-informed gated residual UNet trained through curriculum learning with potential domain decomposition for the largest crystal counts. This combination leverages each strategy's strengths while mitigating individual weaknesses, providing the best path toward reliable generalization across crystal counts.