% ----------------------------------------------------------------------------
% Chapter 6: Experiments
% ----------------------------------------------------------------------------
%
% ============================================================
% QUELLENVORSCHLÄGE FÜR DIESES KAPITEL
% ============================================================
% Experimentelles Setup / Hyperparameter:
%   \parencite{thuerey_deep_2020}            -- Referenz für typische U-Net-Trainingsparameter
%   \parencite{li_fourier_2021}              -- FNO-Hyperparameter (k_max, d_v, Schichtanzahl)
%   \parencite{margenberg_structure_2021}    -- Vergleichspunkt für Neural-Multigrid-Setup
%
% Fehlermetriken (MSE, relL2, Exceedance Fractions):
%   \parencite{takamoto_pdebench_nodate}     -- PDEBench: Standard-Metriken für PDE-Surrogate
%   \parencite{lu_comprehensive_2022}        -- Faire Vergleichsmethodik für Neural Operators
%   \parencite{morimoto_generalization_2022} -- Stratifizierung von Fehlern nach Geometriekomplexität
%
% Lernaufgabe / Stream-Function als Lernziel:
%   \parencite{richter-powell_neural_nodate} -- Divergenzfreiheit durch Output-Repräsentation
%   \parencite{ribeiro_deepcfd_nodate}       -- MSE-Verlust auf normalisiertem Strömungsfeld
%
% Mehrkristall-Generalisierung:
%   \parencite{rana_scalable_2024}               -- Generalisierung auf mehr Hindernisse als im Training
%   \parencite{sanchez-gonzalez_learning_nodate} -- Interaktionsregime in Mehrteilchensystemen
% ============================================================
%
\chapter{Experiments}
\label{ch:experiments}


Das Kapitel muss umgeschrieben werden. Ich will hier einmal kuz erklären was genau ich untersuche und vergleichen habe etc. Kaptiel 7 wird gestrichen. Die Ergbnisse aus meinen Experiementen werden dann in Kapitel 8 präsentiert und diskutiert. In diesem Kapitel soll es also um die Beschreibung der Experimente gehen, die ich durchgeführt habe, um die Leistung meines Modells zu bewerten. Ich werde erklären, welche Metriken ich verwende, um die Vorhersagen meines Modells mit den Referenzlösungen zu vergleichen, und wie ich die Ergebnisse interpretiere. Es wird auch eine Diskussion darüber geben, welche Aspekte meiner Modellierung gut funktionieren und wo es noch Herausforderungen gibt.

Dinge die hier rein sollen:
- Trainings- und Testdaten: Wie viele Samples, wie wurden sie generiert, welche Aufteilung zwischen Training und Test, etc.
- Lernaufgabe: Was genau soll das Modell lernen? Welche Eingaben und Ausgaben hat es? Wie wird die physikalische Konsistenz (z.B. Inkompressibilität) gewährleistet?
- Fehlermaße: Welche Metriken verwende ich, um die Vorhersagen meines Modells mit den Referenzlösungen zu vergleichen? Zum Beispiel MSE, relative Fehler, Exceedance-Fractions, etc. Warum habe ich diese Metriken gewählt und was sagen sie über die Qualität der Vorhersagen aus?
- Überblick über die Experimente: Welche verschiedenen Konfigurationen habe ich getestet (z.B. verschiedene Hyperparameter, verschiedene Trainingsgrößen, etc.)? 
Wie habe ich die Experimente strukturiert, um die Leistung meines Modells systematisch zu bewerten?

