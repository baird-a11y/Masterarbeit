% ----------------------------------------------------------------------------
% Chapter 6: Experiments
% ----------------------------------------------------------------------------
\chapter{Results and Discussion}
\label{ch:experiments}

This chapter presents the empirical performance of the surrogate modelling
framework for \textbf{Approach~2} (stream–function prediction). We first report
a structured hyperparameter study for the \textbf{single–crystal} setting,
identify a robust configuration, and then evaluate the same modelling approach
on \textbf{two–crystal} configurations to assess initial generalization behaviour.

% ----------------------------------------------------------------------------
\section{Experimental Setup}
\label{sec:exp_setup}
% ----------------------------------------------------------------------------

All experiments were run in Julia using Flux v0.16.5 and CUDA v5.9.3 on a GPU.
The surrogate model is the U-Net architecture described in
Chapters~\ref{ch:methodology} and~\ref{ch:implementation}.
Training follows the implementation in Chapter~\ref{ch:implementation}:
gradients are computed with automatic differentiation and parameters are updated
using the Adam optimizer with adaptive learning rates.

Across the runs reported in this chapter, training durations range from
100 to 800 epochs, batch sizes range from 2 to 16, and learning rates were
varied between $10^{-6}$ and $3\cdot 10^{-4}$. Unless stated otherwise, the loss
function is the mean-squared error (MSE) on the normalized stream function
$\psi_{\mathrm{norm}}$.

\subsection{Learning task and reconstruction of velocity}
\label{sec:learning_task}

The network predicts the \emph{normalized} stream function
$\psi_{\mathrm{norm}}$ from a 5-channel input consisting of
(i) the crystal mask,
(ii) a signed distance field (SDF),
(iii) the distance to the nearest crystal boundary,
(iv) normalized $x$ coordinates, and
(v) normalized $z$ coordinates.
The physical stream function $\psi$ is reconstructed using the sample-wise
scaling factor stored with each dataset entry (see Chapter~\ref{ch:implementation}).

For two-dimensional incompressible flows, the velocity field can be recovered
from $\psi$ via
\begin{equation}
u_x = \frac{\partial \psi}{\partial z}, \qquad
u_z = -\frac{\partial \psi}{\partial x},
\label{eq:stream_to_velocity}
\end{equation}
so that incompressibility is satisfied by construction. In practice, this means
that the learning task focuses on reproducing a single scalar field, while the
vector-valued velocity field is obtained deterministically by differentiation.

% ----------------------------------------------------------------------------
\subsection{Error metrics}
\label{sec:error_metrics}
% ----------------------------------------------------------------------------

To quantify prediction quality, we compare the predicted field $\hat{\psi}$ to
the reference solution $\psi$ on the discrete $256\times 256$ grid. Let $\Omega$
denote the set of all pixels and $|\Omega|$ its cardinality.

\paragraph{Mean-squared error (MSE).}
The mean-squared error measures the average squared deviation per pixel:
\begin{equation}
\mathrm{MSE}(\psi)
=
\frac{1}{|\Omega|}
\sum_{i\in\Omega}
\left(\hat{\psi}_i-\psi_i\right)^2 .
\label{eq:mse_def}
\end{equation}
Because deviations are squared, MSE places a stronger emphasis on large local
errors. It serves as a stable global indicator in absolute units, but it
depends on the magnitude (scaling) of $\psi$.

\paragraph{Relative $L_2$ error.}
To obtain a scale-invariant metric, we report the relative $L_2$ error:
\begin{equation}
\mathrm{rel}L_2(\psi)
=
\frac{\|\hat{\psi}-\psi\|_2}{\|\psi\|_2},
\qquad
\|a\|_2=\left(\sum_{i\in\Omega} a_i^2\right)^{1/2}.
\label{eq:rell2}
\end{equation}
Unlike MSE, $\mathrm{rel}L_2$ normalizes by the energy of the reference field
$\|\psi\|_2$, making it comparable across samples whose flow intensity differs
(e.g.\ due to crystal position or interaction strength). For example,
$\mathrm{rel}L_2(\psi)=0.02$ indicates that the $L_2$ magnitude of the error is
about 2\% of the $L_2$ magnitude of the reference field.

\paragraph{Threshold exceedance fractions $\epsilon_\tau$.}
Global norms can be small even when localised regions contain noticeable errors.
Therefore, we also compute the fraction of pixels whose \emph{pointwise relative
error} exceeds a threshold $\tau$:
\begin{equation}
\epsilon_{\tau}(\psi)
=
\frac{1}{|\Omega|}
\left|
\left\{
i\in\Omega :
e_i(\psi)>\tau
\right\}
\right|,
\qquad
e_i(\psi)=
\frac{|\hat{\psi}_i-\psi_i|}{|\psi_i|+\delta}.
\label{eq:eps_def}
\end{equation}
Here, $\delta$ is a small stabilizing constant to prevent division by values
close to zero. We report $\epsilon_{0.01}$, $\epsilon_{0.05}$, and
$\epsilon_{0.10}$, i.e.\ the fraction of pixels with local relative errors
above 1\%, 5\%, and 10\%. These metrics help distinguish whether errors are
confined to localized regions (e.g.\ near boundaries or strong gradients) or
spread across the full domain. Importantly, exceedance fractions can remain
comparatively high in regions where $\psi$ is small in magnitude, since the
relative error is sensitive to small denominators.

\paragraph{Metrics for derived quantities ($\psi_x$, $\psi_z$).}
Since velocities are obtained from $\psi$ via
Equation~\eqref{eq:stream_to_velocity}, we also apply the same metrics to the
spatial derivatives $\psi_x$ and $\psi_z$. This provides a stricter assessment:
numerical differentiation tends to amplify high-frequency noise, so small
pixel-scale fluctuations in $\hat{\psi}$ can translate into larger discrepancies
in $\psi_x$ and $\psi_z$. Consequently, gradient-based errors are expected to be
larger than errors in $\psi$ itself.

% ----------------------------------------------------------------------------
\section{Single–Crystal Hyperparameter Study}
\label{sec:results_single_crystal}
% ----------------------------------------------------------------------------

A set of 16 training runs was performed for $N=1$ crystal to study the influence
of batch size, learning rate, dataset size, and training duration. All runs used
MSE training loss on $\psi_{\mathrm{norm}}$.

\subsection{Overview of explored settings}

The study covered:
\begin{itemize}
    \item \textbf{training set size:} 2000 vs.\ 4000 samples,
    \item \textbf{batch size:} $2,4,8,16$,
    \item \textbf{learning rate:} $10^{-6}$ to $3\cdot 10^{-4}$,
    \item \textbf{epochs:} 100 to 800.
\end{itemize}
In addition, a small number of exploratory runs used a changing learning rate,
but the core sweep results reported below are based on fixed learning rates to
maintain comparability across runs.

\subsection{Best configuration for one crystal}

Across all single–crystal runs, the strongest performance was obtained by:
\begin{quote}
\textbf{Run\_15:} 4000 samples, learning rate $5\cdot 10^{-5}$, 300 epochs,
batch size 8.
\end{quote}

This configuration achieved
\[
\mathrm{MSE}(\psi)\approx 2.70\cdot 10^{-26}, \qquad
\mathrm{rel}L_2(\psi)\approx 2.09\cdot 10^{-2},
\]
and also strong performance for gradient-derived quantities
($\mathrm{rel}L_2(\psi_x)\approx 0.166$, $\mathrm{rel}L_2(\psi_z)\approx 0.187$),
indicating that the model not only matches the global stream-function structure
but also reproduces velocity-relevant spatial variations comparatively well.

Table~\ref{tab:single_crystal_key_runs} summarizes representative runs and the
best configuration.

\begin{table}[t]
\centering
\caption{Representative single–crystal results (Approach~2).}
\label{tab:single_crystal_key_runs}
\begin{tabular}{lrrrrrr}
\hline
Run & Samples & LR & Epochs & Batch & $\mathrm{MSE}(\psi)$ & $\mathrm{rel}L_2(\psi)$ \\
\hline
Run\_1  & 2000 & $10^{-4}$       & 500 & 2  & $1.62\cdot 10^{-23}$ & 0.300 \\
Run\_3  & 2000 & $10^{-4}$       & 500 & 8  & $2.30\cdot 10^{-25}$ & 0.0500 \\
Run\_10 & 4000 & $10^{-4}$       & 700 & 8  & $1.26\cdot 10^{-24}$ & 0.0969 \\
Run\_11 & 4000 & $5\cdot 10^{-5}$& 800 & 8  & $3.70\cdot 10^{-25}$ & 0.0348 \\
\textbf{Run\_15} & \textbf{4000} & $\mathbf{5\cdot 10^{-5}}$ & \textbf{300} & \textbf{8}
& $\mathbf{2.70\cdot 10^{-26}}$ & $\mathbf{0.0209}$ \\
Run\_16 & 4000 & $5\cdot 10^{-5}$& 500 & 8  & $1.43\cdot 10^{-25}$ & 0.0408 \\
\hline
\end{tabular}
\end{table}

\subsection{Trends and practical takeaways}

Three practical patterns emerge:

\begin{itemize}
    \item \textbf{Batch size has a strong effect.}
    For otherwise identical settings (2000 samples, LR=$10^{-4}$, 500 epochs),
    batch size 8 outperformed batch sizes 2, 4, and 16. This suggests that in
    this problem, an intermediate batch size provides a favourable trade-off
    between gradient noise (too small batches) and overly smooth updates (too
    large batches).

    \item \textbf{Moderate learning rates are most reliable.}
    Very small learning rates (e.g.\ $10^{-6}$) underperformed in the available
    training budget, consistent with under-training. Conversely, larger learning
    rates ($\ge 2\cdot 10^{-4}$) did not yield improvements and can lead to
    unstable or overly coarse optimisation steps for this architecture and loss.

    \item \textbf{More epochs are not automatically better.}
    Run\_15 (300 epochs) outperformed longer trainings at similar learning rate
    (e.g.\ Run\_16 at 500 epochs and Run\_11 at 800 epochs). A plausible
    interpretation is that after a certain point, additional training can
    prioritise fitting sample-specific details that do not translate into better
    generalisation on the held-out test set. In this sense, the selected
    configuration can be viewed as a good ``early-stopped'' solution in terms of
    test performance.
\end{itemize}

Finally, threshold exceedance metrics (e.g.\ $\epsilon_{0.01}$) remain
comparatively high even for the best run. This is consistent with the nature of
relative pointwise errors: pixels with small $|\psi|$ can dominate exceedance
counts, even if absolute deviations are tiny and the global norm errors are low.

% ----------------------------------------------------------------------------
\section{Two–Crystal Evaluation}
\label{sec:results_two_crystal}
% ----------------------------------------------------------------------------

To assess an initial step towards multi-crystal capability, the stream-function
model was evaluated on configurations containing $N=2$ crystals. Compared to the
single-crystal regime, two-crystal configurations introduce interaction effects
that can alter streamline topology and create sharper spatial variations,
thereby providing a more stringent test of the surrogate.

\subsection{Aggregate performance for $N=1$ vs.\ $N=2$}

A grouped evaluation over 100 test samples per crystal count yields the
statistics in Table~\ref{tab:two_crystal_grouped}.

\begin{table}[t]
\centering
\caption{Grouped test errors for one- and two-crystal configurations (100 samples each).}
\label{tab:two_crystal_grouped}
\begin{tabular}{lrrrr}
\hline
$N$ & $\mathrm{MSE}(\psi)$ mean & $\mathrm{MSE}(\psi)$ std
& $\mathrm{rel}L_2(\psi)$ mean & $\mathrm{rel}L_2(\psi)$ std \\
\hline
1 & $2.31\cdot 10^{-25}$ & $2.28\cdot 10^{-25}$ & 0.0554 & 0.0315 \\
2 & $1.27\cdot 10^{-24}$ & $3.81\cdot 10^{-24}$ & 0.0765 & 0.0429 \\
\hline
\end{tabular}
\end{table}

Moving from $N=1$ to $N=2$ increases the mean error, as expected when evaluating
outside the simpler regime. Notably, the two-crystal MSE exhibits a much larger
standard deviation, indicating that prediction difficulty varies substantially
across samples. This behaviour is consistent with the idea that geometry
matters: the relative placement of crystals can create either weak interactions
(easier cases) or strong interactions with sharper gradients and more complex
streamline structures (harder cases).

\subsection{Spatial error dependence: distance to center and corners}

A diagnostic evaluation stores, per crystal instance, its distance to the domain
center and to the four domain corners, together with the corresponding
prediction error. While the present chapter does not yet include the
corresponding figures, the recorded data supports two qualitative observations:

\begin{itemize}
    \item For \textbf{single crystals}, errors tend to increase when the crystal
    is far from the domain center (equivalently, closer to boundaries/corners).
    A plausible explanation is that the flow structure in these cases is more
    strongly influenced by boundary conditions, and the effective fraction of
    ``unstructured'' fluid region changes as the crystal approaches the domain
    edges.

    \item For \textbf{two crystals}, the error distribution becomes broader and
    exhibits more outliers. This is consistent with interaction regimes in which
    the two crystals generate localised zones of stronger gradients and more
    complex streamline topology, which are harder to reproduce accurately with a
    purely data-driven surrogate.
\end{itemize}

A natural next step is to visualise these dependencies explicitly (scatter plots
and binned averages) for $N=1$ and $N=2$ separately, and to relate high-error
outliers to geometric descriptors such as minimum inter-crystal distance and
boundary proximity.

% ----------------------------------------------------------------------------
\section{Discussion}
\label{sec:results_discussion}
% ----------------------------------------------------------------------------

\subsection{What works well}

The results demonstrate that Approach~2 can learn the single-crystal mapping
with high accuracy in global norms. In particular, the best configuration
achieves $\mathrm{rel}L_2(\psi)\approx 2\%$ for $N=1$ without additional
physics-informed loss terms. Because velocities are derived from a stream
function, the approach preserves incompressibility by construction
(Equation~\ref{eq:stream_to_velocity}), which is a practical advantage: the model
does not need to learn a divergence-free constraint explicitly.

\subsection{Where errors concentrate and why gradients are harder}

Two recurring patterns appear in the evaluation:
\begin{itemize}
    \item \textbf{Gradient-derived quantities are more challenging.}
    Even when $\psi$ is reconstructed accurately in a global sense, small
    pixel-scale deviations can translate into visibly larger discrepancies in
    $\psi_x$ and $\psi_z$, because differentiation amplifies high-frequency noise.
    Consequently, gradient errors act as a stricter test for capturing fine-scale
    flow features that directly affect the velocity field.

    \item \textbf{Geometry-dependent difficulty.}
    Two-crystal configurations produce a wider spread of errors, indicating that
    the surrogate’s performance depends on the interaction regime. Configurations
    with strong hydrodynamic coupling or pronounced local gradients are expected
    to be harder than cases where crystals are well separated or positioned in a
    more symmetric manner.
\end{itemize}

\subsection{Implications for the multi-crystal goal}

The increase in mean error from $N=1$ to $N=2$ provides a first estimate of the
generalisation gap when moving to more complex geometries. The substantially
higher variance for $N=2$ suggests that ``average'' metrics alone are not
sufficient to characterise performance; instead, evaluations should be
stratified by geometric descriptors. Two particularly actionable directions are:
(i) reporting errors as a function of boundary distance and minimum inter-crystal
distance, and (ii) expanding the training distribution to include multi-crystal
samples so that interaction patterns are learned directly rather than
extrapolated.

% ----------------------------------------------------------------------------
\section{Summary}
\label{sec:results_summary}
% ----------------------------------------------------------------------------

This chapter evaluated the stream-function surrogate (Approach~2) on single- and
two-crystal configurations. A hyperparameter study for $N=1$ identified an
effective configuration (4000 samples, learning rate $5\cdot10^{-5}$, batch size
8, 300 epochs) achieving $\mathrm{rel}L_2(\psi)\approx 0.021$. When evaluating on
$N=2$ crystals, the mean error increased to $\mathrm{rel}L_2(\psi)\approx 0.076$
with a substantially larger variance across samples. Distance-based diagnostics
suggest geometry-dependent difficulty, motivating a structured generalisation
analysis for $N\ge 2$ and targeted dataset enrichment in subsequent work.
