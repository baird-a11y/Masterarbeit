% ----------------------------------------------------------------------------
% Chapter 6: Experiments
% ----------------------------------------------------------------------------
%
% ============================================================
% QUELLENVORSCHLÄGE FÜR DIESES KAPITEL
% ============================================================
% Experimentelles Setup / Hyperparameter:
%   \parencite{thuerey_deep_2020}            -- Referenz für typische U-Net-Trainingsparameter
%   \parencite{li_fourier_2021}              -- FNO-Hyperparameter (k_max, d_v, Schichtanzahl)
%   \parencite{margenberg_structure_2021}    -- Vergleichspunkt für Neural-Multigrid-Setup
%
% Fehlermetriken (MSE, relL2, Exceedance Fractions):
%   \parencite{takamoto_pdebench_nodate}     -- PDEBench: Standard-Metriken für PDE-Surrogate
%   \parencite{lu_comprehensive_2022}        -- Faire Vergleichsmethodik für Neural Operators
%   \parencite{morimoto_generalization_2022} -- Stratifizierung von Fehlern nach Geometriekomplexität
%
% Lernaufgabe / Stream-Function als Lernziel:
%   \parencite{richter-powell_neural_nodate} -- Divergenzfreiheit durch Output-Repräsentation
%   \parencite{ribeiro_deepcfd_nodate}       -- MSE-Verlust auf normalisiertem Strömungsfeld
%
% Mehrkristall-Generalisierung:
%   \parencite{rana_scalable_2024}               -- Generalisierung auf mehr Hindernisse als im Training
%   \parencite{sanchez-gonzalez_learning_nodate} -- Interaktionsregime in Mehrteilchensystemen
% ============================================================
%
\chapter{Experiments}
\label{ch:experiments}

This chapter describes the experimental setup used to evaluate and compare the U-Net and FNO surrogate models. It specifies the dataset, the learning task, the evaluation metrics, and the set of experiments designed to assess model accuracy and generalization.

% ----------------------------------------------------------------------------
\section{Dataset and Data Split}
\label{sec:experiments:dataset}
% ----------------------------------------------------------------------------

All training and evaluation data are generated using LaMEM, a parallel finite-difference Stokes solver. Each simulation produces a steady-state Stokes flow in a $[-1,1]^2$ domain containing one or more circular crystals with fixed viscosity contrast. The stream function $\psi$ is reconstructed from the vorticity output via the Poisson solver described in Chapter~\ref{ch:implementation}, and normalized per sample before storage.

Samples are stored as individual \texttt{.jld2} files, each containing the input tensor, the normalized target field $\psi_{\mathrm{norm}}$, the normalization scale, and grid metadata. Data generation is executed serially to guarantee deterministic behavior.

The dataset is stratified by crystal count. Samples with $n = 1, 2, 3$ crystals are used for training and validation; generalization to higher crystal counts is evaluated on held-out test samples. The train/validation split is 80/20 within each stratum, ensuring balanced representation across geometric complexities.

% ----------------------------------------------------------------------------
\section{Learning Task and Input Encoding}
\label{sec:experiments:task}
% ----------------------------------------------------------------------------

Both models learn the mapping from a geometric encoding of the crystal configuration to the normalized stream function $\psi_{\mathrm{norm}}$. The stream function is chosen as the learning target because it implicitly encodes the incompressibility constraint: the velocity field $\mathbf{u} = (\partial\psi/\partial z,\, -\partial\psi/\partial x)$ is divergence-free by construction.

Each input sample consists of four channels on a $256 \times 256$ Cartesian grid:
\begin{enumerate}
    \item \textbf{Crystal mask} — binary indicator of solid crystal regions ($1$ inside, $0$ outside).
    \item \textbf{Signed distance field (SDF)} — signed distance to the nearest crystal boundary, providing smooth geometric information beyond the binary mask.
    \item \textbf{Normalized $x$-coordinate} — grid position along the horizontal axis, scaled to $[-1, 1]$.
    \item \textbf{Normalized $z$-coordinate} — grid position along the vertical axis, scaled to $[-1, 1]$.
\end{enumerate}

The coordinate channels allow the network to resolve position-dependent flow patterns without relying on positional embeddings. The single output channel is $\psi_{\mathrm{norm}} \in \mathbb{R}^{256 \times 256 \times 1}$; rescaling to physical units is applied in post-processing using the stored per-sample scale factor.

% ----------------------------------------------------------------------------
\section{Evaluation Metrics}
\label{sec:experiments:metrics}
% ----------------------------------------------------------------------------

Predictions are evaluated on the normalized stream function $\psi_{\mathrm{norm}}$ as well as on derived velocity fields $\mathbf{u} = (\partial\psi/\partial z,\, -\partial\psi/\partial x)$, computed via finite differences on the $256 \times 256$ grid. The following metrics are reported:

\begin{description}
    \item[MSE] Mean squared error on $\psi_{\mathrm{norm}}$, measuring overall pointwise accuracy.
    \item[Relative $L_2$ error] $\|\hat{\psi} - \psi\|_2 / \|\psi\|_2$, providing a scale-invariant measure of generalization quality.
    \item[Maximum absolute error] $\|\hat{\psi} - \psi\|_\infty$, quantifying worst-case local deviations.
    \item[Velocity MSE] MSE on the derived velocity components $u_x$ and $u_z$, assessing the physical consistency of the predicted stream function beyond pointwise $\psi$ accuracy.
    \item[Gradient MSE] (FNO only) MSE on spatial derivatives $\partial\psi/\partial x$ and $\partial\psi/\partial z$, which directly corresponds to the gradient loss term used during FNO training.
    \item[Boundary MSE] (FNO only) MSE evaluated only on the domain boundary $\partial\Omega$, measuring compliance with the homogeneous Dirichlet condition $\psi|_{\partial\Omega} = 0$.
\end{description}

All metrics are aggregated by crystal count to assess how performance scales with geometric complexity. This stratification is essential for distinguishing in-distribution accuracy from out-of-distribution generalization.

% ----------------------------------------------------------------------------
\section{Experimental Configurations}
\label{sec:experiments:configs}
% ----------------------------------------------------------------------------

Four experiments are conducted to systematically evaluate and compare both models.
Für alle Experimente wurden folgenden Standart Konfigurationen ausgewählt für die Architekturen:

Übersicht der Konfigurationen einfügen




\paragraph{Experiment 1 – Single-crystal baseline.}
Both models are trained and evaluated on configurations with exactly one crystal ($n = 1$). This experiment establishes a baseline for each architecture under the simplest geometric setting and provides a controlled comparison of their accuracy and training behavior.
Hierbei wurden jeweils vier Unterschiedliche lernraten verwendet (1e-3, 5e-3, 1e-4,5e-4) sowie einmal mit Batchsize 16 und 8 und verglichen.

\paragraph{Experiment 2 – Multi-crystal generalization.}
Models are trained on samples with $n \in \{1, 2, 10\}$ crystals and evaluated separately on each crystal count and how well they perform on unseen higher crystal counts ($n = 11, ... , 25$). This experiment assesses the ability of each architecture to generalize to more complex geometries than those seen during training, which is critical for practical applications in geodynamics where the number of crystals can vary widely.

\paragraph{Experiment 3 – Limitations.}
Die Modelle werden für höhrere Kirstallkonfigurationen trainiert (n bis zu 25) und anschließend soll auf eine wesentlich höhere Anzahl von Kristallen (n bis zu 100) generalisiert werden. Hierbei soll untersucht werden, ob die Modelle in der Lage sind, die Interaktionen zwischen einer großen Anzahl von Hindernissen zu erfassen, oder ob sie an einem Punkt der Komplexität überfordert sind.

\paragraph{Experiment 4 – Changing Size (Suche besseren Namen).}
Die Modelle werden wie in Experiment 2 trainiert, aber zusätzlich werden die Kristalle in den evaluierungs proben anders skaliert (um den Faktor 10 nach ob und unten). Dies testet die Fähigkeit der Modelle, nicht nur auf mehr Kristalle zu generalisieren, sondern auch auf größere Hindernisse, was eine weitere Dimension der Generalisierung darstellt.

% ----------------------------------------------------------------------------
\section{Training Setup and Hyperparameters}
\label{sec:experiments:setup}
% ----------------------------------------------------------------------------

Both models are trained on the same datasets and evaluated with the same metrics. Table~\ref{tab:hyperparams} summarizes the key hyperparameters.

\begin{table}[h]
\centering
\caption{Hyperparameter comparison of U-Net and FNO training configurations.}
\label{tab:hyperparams}
\begin{tabular}{lll}
\toprule
\textbf{Parameter}       & \textbf{U-Net}              & \textbf{FNO}                          \\
\midrule
Optimizer                & Adam                        & Adam + ClipNorm                        \\
Learning rate            & $1 \times 10^{-3}$ to $5 \times 10^{-4}$          & $1 \times 10^{-3}$ to $5 \times 10^{-4}$                              \\
Max.\ gradient norm      & ---                         & 1.0                                    \\
Batch size               & 8/16                           & 8/16                                     \\
Epochs                   & 50                        & 50                                    \\
Input channels           & 4                           & 4                                      \\
Output channels          & 1                           & 1                                      \\
Base channels / width    & 32                          & $d_v = 64$                             \\
Architecture depth       & 4 encoder levels            & $L = 4$ FNO blocks                     \\
Fourier modes $k_{\max}$ & ---                         & 24                                     \\
Checkpoint format        & BSON                        & JLD2                                   \\
\bottomrule
\end{tabular}
\end{table}

Training is performed on a single GPU; models are saved in CPU-compatible format to enable evaluation on systems without GPU support.
